{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using a Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that and is CC, a coordinating conjunction; now and completely are RB, or adverbs; for is IN, a preposition; something is NN, a noun; and different is JJ, an adjective.\n",
    "\n",
    "**Note-:** NLTK provides documentation for each tag, which can be queried using the tag, e.g. nltk.help.upenn_tagset('RB'), or a regular expression, e.g. nltk.help.upenn_tagset('NN.*'). Some corpora have README files with tagset documentation, see nltk.corpus.???.readme(), substituting in the name of the corpus.\n",
    "\n",
    "Let's look at another example, this time including some homonyms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that refuse and permit both appear as a present tense verb (VBP) and a noun (NN). E.g. refUSE is a verb meaning \"deny,\" while REFuse is a noun meaning \"trash\" (i.e. they are not homophones). Thus, we need to know which word is being used in order to pronounce the text correctly. (For this reason, text-to-speech systems usually perform POS-tagging.)\n",
    "\n",
    "**Note-:** Your Turn: Many words, like ski and race, can be used as nouns or verbs with no difference in pronunciation. Can you think of others? Hint: think of a commonplace object and try to put the word to before it to see if it can also be a verb, or think of an action and try to put the before it to see if it can also be a noun. Now make up a sentence with both uses of this word, and run the POS-tagger on this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world family house country child boy\n",
      "state job way war girl place word work\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made said put done seen had found left given heard brought got been\n",
      "was set told took in felt that\n"
     ]
    }
   ],
   "source": [
    "text.similar('bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in on to of and for with from at by that into as up out down through\n",
      "is all about\n"
     ]
    }
   ],
   "source": [
    "text.similar('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a his this their its her an that our any all one these my in your no\n",
      "some other and\n"
     ]
    }
   ],
   "source": [
    "text.similar('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that searching for woman finds nouns; searching for bought mostly finds verbs; searching for over generally finds prepositions; searching for the finds several determiners. A tagger can correctly identify the tags on these words in the context of a sentence, e.g. The woman bought over $150,000 worth of clothes.\n",
    "\n",
    "A tagger can also model our knowledge of unknown words, e.g. we can guess that scrobbling is probably a verb, with the root scrobble, and likely to occur in contexts like he was scrobbling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tagged Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1   Representing Tagged Tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention in NLTK, a tagged token is represented using a tuple consisting of the token and the tag. We can create one of these special tuples from the standard string representation of a tagged token, using the function str2tuple():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fly', 'NN')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
    "tagged_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fly'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_token[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a list of tagged tokens directly from a string. The first step is to tokenize the string to access the individual word/tag strings, and then to convert each of these into a tuple (using str2tuple())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Doing', None),\n",
       " ('the', None),\n",
       " ('assignment', None),\n",
       " ('of', None),\n",
       " ('NLP', None),\n",
       " ('is', None),\n",
       " ('fun.', None),\n",
       " ('This', None),\n",
       " ('is', None),\n",
       " ('the', None),\n",
       " ('assignment', None),\n",
       " ('on', None),\n",
       " ('POS', None),\n",
       " ('tagging', None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Doing the assignment of NLP is fun. This is the assignment on POS tagging\"\n",
    "[nltk.tag.str2tuple(t) for t in sent.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1   Reading Tagged Corpora\n",
    "\n",
    "Several of the corpora included with NLTK have been tagged for their part-of-speech. Here's an example of what you might see if you opened a file from the Brown Corpus with a text editor:\n",
    "\n",
    "The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd / no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
    "\n",
    "Other corpora use a variety of formats for storing part-of-speech tags. NLTK's corpus readers provide a uniform interface so that you don't have to be concerned with the different file formats. In contrast with the file fragment shown above, the corpus reader for the Brown Corpus represents the data as shown below. Note that part-of-speech tags have been converted to uppercase, since this has become standard practice since the Brown Corpus was published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', u'AT'), (u'Fulton', u'NP-TL'), ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'The', u'DET'), (u'Fulton', u'NOUN'), ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a corpus contains tagged text, the NLTK corpus interface will have a tagged_words() method. Here are some more examples, again using the output format illustrated for the Brown Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Confidence', u'NN'), (u'in', u'IN'), ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.conll2000.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Pierre', u'NNP'), (u'Vinken', u'NNP'), ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.treebank.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all corpora employ the same set of tags; see the tagset help functionality and the readme() methods mentioned above for documentation. Initially we want to avoid the complications of these tagsets, so we use a built-in mapping to the \"Universal Tagset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Pierre', u'NOUN'), (u'Vinken', u'NOUN'), ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.treebank.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagged corpora for several other languages are distributed with NLTK, including Chinese, Hindi, Portuguese, Spanish, Dutch and Catalan. These usually contain non-ASCII text, and Python always displays this in hexadecimal when printing a larger structure such as a list. \n",
    "\n",
    "TO check more tagged corpora, type `nltk.download()` and browse through the corpora section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Models in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll show how the Viterbi algorithm works for HMMs, assuming we have a trained model to start with. Further down we look at the forward and backward algorithms and Baum-Welch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model parameters based on the example from the lecture slides<br>\n",
    "http://people.eng.unimelb.edu.au/tcohn/comp90042/l15.ppt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d97fcc674529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Data/parameters.JPG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "Image(filename='Data/parameters.JPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([[0.6, 0.2, 0.2], [0.5, 0.3, 0.2], [0.4, 0.1, 0.5]])\n",
    "pi = np.array([0.5, 0.2, 0.3])\n",
    "O = np.array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3], [0.3, 0.3, 0.4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider the example observation sequence UP, UP, DOWN, for which we'll try to discover the hidden state sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = UP, DOWN, UNCHANGED = 0, 1, 2\n",
    "observations = [UP, UP, DOWN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll code the Viterbi algorithm. It keeps a store of two components, the best scores to reach a state at a give time, and the last step of the path to get there. Scores alpha are initialised to -inf to denote that we haven't set them yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = np.zeros((len(observations), len(states))) # time steps x states\n",
    "alpha[:,:] = float('-inf')\n",
    "backpointers = np.zeros((len(observations), len(states)), 'int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base case for the recursion sets the starting state probs based on pi and generating the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# base case, time step 0\n",
    "alpha[0, :] = pi * O[:,UP]\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the recursive step, where we maximise over incoming transitions reusing the best incoming score, computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# time step 1\n",
    "for t1 in states:\n",
    "    for t0 in states:\n",
    "        score = alpha[0, t0] * A[t0, t1] * O[t1,UP]\n",
    "        if score > alpha[1, t1]:\n",
    "            alpha[1, t1] = score\n",
    "            backpointers[1, t1] = t0\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat with the next observation. (We'd do this as a loop in practice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# time step 2\n",
    "for t2 in states:\n",
    "    for t1 in states:\n",
    "        score = alpha[1, t1] * A[t1, t2] * O[t2,DOWN]\n",
    "        if score > alpha[2, t2]:\n",
    "            alpha[2, t2] = score\n",
    "            backpointers[2, t2] = t1\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read of the best final state, and follow the backpointers to recover the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(alpha[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backpointers[2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backpointers[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew. The best state sequence is [0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalising things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put this all into a function to handle arbitrary length inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf')\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                score = alpha[t-1, s1] * A[s1, s2] * O[s2, observations[t]]\n",
    "                if score > alpha[t, s2]:\n",
    "                    alpha[t, s2] = score\n",
    "                    backpointers[t, s2] = s1\n",
    "    \n",
    "    # now follow backpointers to resolve the state sequence\n",
    "    ss = []\n",
    "    ss.append(np.argmax(alpha[M-1,:]))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append(backpointers[i, ss[-1]])\n",
    "        \n",
    "    return list(reversed(ss)), np.max(alpha[M-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viterbi((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viterbi((pi, A, O), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that we've done the above algorithm correctly by implementing exhaustive search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def exhaustive(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    # track the running best sequence and its score\n",
    "    best = (None, float('-inf'))\n",
    "    # loop over the cartesian product of |states|^M\n",
    "    for ss in product(range(S), repeat=M):\n",
    "        # score the state sequence\n",
    "        score = pi[ss[0]] * O[ss[0],observations[0]]\n",
    "        for i in range(1, M):\n",
    "            score *= A[ss[i-1], ss[i]] * O[ss[i], observations[i]]\n",
    "        # update the running best\n",
    "        if score > best[1]:\n",
    "            best = (ss, score)\n",
    "            \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exhaustive((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exhaustive((pi, A, O), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, it got the same results as before. Note that the exhaustive method is practical on anything beyond toy data due to the nasty cartesian product. But it is worth doing to verify the Viterbi code above is getting the right results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised training, aka \"visible\" Markov model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the HMM parameters on the Penn Treebank, using the sample from NLTK. Note that this is a small fraction of the treebank, so we shouldn't expect great performance of our method trained only on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = treebank.tagged_sents()\n",
    "print corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_text = nltk.corpus.brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brownSent = \"\"\n",
    "for word in news_text[0:20]:\n",
    "    brownSent = brownSent + str(word)\n",
    "    brownSent = brownSent + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first map words and tags to numbers for compatibility with the above methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start1 = datetime.now()\n",
    "\n",
    "word_numbers = {}\n",
    "tag_numbers = {}\n",
    "\n",
    "num_corpus = []\n",
    "for sent in corpus:\n",
    "    num_sent = []\n",
    "    for word, tag in sent:\n",
    "        wi = word_numbers.setdefault(word.lower(), len(word_numbers))\n",
    "        ti = tag_numbers.setdefault(tag, len(tag_numbers))\n",
    "        num_sent.append((wi, ti))\n",
    "    num_corpus.append(num_sent)\n",
    "    \n",
    "word_names = [None] * len(word_numbers)\n",
    "for word, index in word_numbers.items():\n",
    "    word_names[index] = word\n",
    "tag_names = [None] * len(tag_numbers)\n",
    "for tag, index in tag_numbers.items():\n",
    "    tag_names[index] = tag\n",
    "#Now let's hold out the last few sentences for testing, so that they are unseen \n",
    "#during training and give a more reasonable estimate of accuracy on fresh text.\n",
    "\n",
    "training = num_corpus[:-10]\n",
    "testing = num_corpus[-10:]\n",
    "\n",
    "#Next we compute relative frequency estimates based on the observed tag\n",
    "#and word counts in the training set. Note that smoothing is important, here we add a small constant to all counts. \n",
    "\n",
    "S = len(tag_numbers)\n",
    "V = len(word_numbers)\n",
    "\n",
    "# initalise\n",
    "eps = 0.1\n",
    "pi = eps * np.ones(S)\n",
    "A = eps * np.ones((S, S))\n",
    "O = eps * np.ones((S, V))\n",
    "\n",
    "# count\n",
    "for sent in training:\n",
    "    last_tag = None\n",
    "    for word, tag in sent:\n",
    "        O[tag, word] += 1\n",
    "        if last_tag != None:\n",
    "            pi[tag] += 1\n",
    "        else:\n",
    "            A[last_tag, tag] += 1\n",
    "        last_tag = tag\n",
    "        \n",
    "# normalise\n",
    "pi /= np.sum(pi)\n",
    "for s in range(S):\n",
    "    O[s,:] /= np.sum(O[s,:])\n",
    "    A[s,:] /= np.sum(A[s,:])\n",
    "#Now we're ready to use our Viterbi method defined above\n",
    "\n",
    "predicted, score = viterbi((pi, A, O), map(lambda (w,t): w, testing[0]))\n",
    "print datetime.now()-start1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '%20s\\t%5s\\t%5s' % ('TOKEN', 'TRUE', 'PRED')\n",
    "for (wi, ti), pi in zip(testing[0], predicted):\n",
    "    print '%20s\\t%5s\\t%5s' % (word_names[wi], tag_names[ti], tag_names[pi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, not bad, only three errors. Can you explain why these might have occurred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginalisation in Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related problem is marginalisation, when we wish to find the probability of an observation sequence *under any hidden state sequence*. This allows hidden Markov models to function as language models, but also is key to unsupervised training and the central algorithm for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Viterbi algorithm, we'll need to start with the mathematical definition and attempt to factorise it (to follow a recursion, thus allowing for dynamic programming). The quantity we wish to compute is $$p(\\vec{w}) = \\sum_{\\vec{t}} p(\\vec{t}, \\vec{w})$$\n",
    "where $w$ are the observations (words) and $t$ are the states (tags)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by expanding the summation \n",
    "$$\n",
    "p(\\vec{w})  = \\sum_{t_1} \\sum_{t_2} \\cdots \\sum_{t_{N-1}} \\sum_{t_N} p(\\vec{t}, \\vec{w})\n",
    "$$\n",
    "and expand the HMM probability \n",
    "$$\n",
    "p(\\vec{w})  = \\sum_{t_1} \\sum_{t_2} \\cdots \\sum_{t_{N-1}} \\sum_{t_N} p(t_1) p(w_1 | t_1) p(t_2 | t_1) p(w_2| t_2) \\cdots p(t_{N-1} | t_{N-2}) p(w_{N-1}| t_{N-1}) p(t_{N} | t_{N-1}) p(w_{N}| t_{N})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the full marginal probability $p(\\vec{w})$ and the probability up to position $N-1$, finishing with tag $t_{N-1}$\n",
    "$$p(w_1, w_2, \\ldots, w_{N-1}, t_{N-1}) = \\sum_{t_1} \\sum_{t_2} \\cdots \\sum_{t_{N-1}} p(t_1) p(w_1 | t_1) p(t_2 | t_1) p(w_2| t_2) \\cdots p(t_{N-1} | t_{N-2}) p(w_{N-1}| t_{N-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look rather similar, and in fact we can express $p(\\vec{w})$ more simply as\n",
    "$$\n",
    "p(\\vec{w})  = \\sum_{t_N} p(w_1, w_2, \\ldots, w_{N-1}, t_{N-1}) p(t_{N} | t_{N-1}) p(w_{N}| t_{N})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue further by defining $p(w_1, w_2, \\ldots, w_{N-1}, t_{N-1})$ in terms of $p(w_1, w_2, \\ldots, w_{N-2}, t_{N-2})$ and so forth. (This is the same process used in the Viterbi algorithm, albeit swapping a max for a sum.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally we store a matrix of partial marginals, $\\alpha$ defined as follows\n",
    "$$\\alpha[i, t_i] = p(w_1, w_2, \\ldots, w_i, t_i)$$\n",
    "computed using the recursion\n",
    "$$  \n",
    "\\alpha[i, t_i] = \\sum_{t_{i-1}} \\alpha[i-1, t_i] p(t_i | t_{i-1}) p(w_i| t_i) \n",
    "$$\n",
    "and the base case for $i=1$,\n",
    "$$\n",
    "\\alpha[1, t_1] = p(t_1) p(w_1 | t_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have computed the formulation, we can put this into an iterative algorithm: compute the vector of alpha[1] values, then alpha[2] etc until we reach the end of our input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward(params, observations):\n",
    "    pi, A, O = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((N, S))\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for i in range(1, N):\n",
    "        for s2 in range(S):\n",
    "            for s1 in range(S):\n",
    "                alpha[i, s2] += alpha[i-1, s1] * A[s1, s2] * O[s2, observations[i]]\n",
    "    \n",
    "    return (alpha, np.sum(alpha[N-1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([[0.6, 0.2, 0.2], [0.5, 0.3, 0.2], [0.4, 0.1, 0.5]])\n",
    "pi = np.array([0.5, 0.2, 0.3])\n",
    "O = np.array([[0.7, 0.1, 0.2], [0.1, 0.6, 0.3], [0.3, 0.3, 0.4]])\n",
    "\n",
    "forward((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi, A, O), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm we did this correctly by implementing an exhaustive equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exhaustive_forward(params, observations):\n",
    "    pi, A, O = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "\n",
    "    total = 0.0\n",
    "    # loop over the cartesian product of |states|^N\n",
    "    for ss in product(range(S), repeat=N):\n",
    "        # score the state sequence\n",
    "        score = pi[ss[0]] * O[ss[0],observations[0]]\n",
    "        for i in range(1, N):\n",
    "            score *= A[ss[i-1], ss[i]] * O[ss[i], observations[i]]\n",
    "        total += score\n",
    "            \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exhaustive_forward((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exhaustive_forward((pi, A, O), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process but working from left to right rather than right to left give us the backward algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward(params, observations):\n",
    "    pi, A, O = params\n",
    "    N = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    beta = np.zeros((N, S))\n",
    "    \n",
    "    # base case\n",
    "    beta[N-1, :] = 1\n",
    "    \n",
    "    # recursive case\n",
    "    for i in range(N-2, -1, -1):\n",
    "        for s1 in range(S):\n",
    "            for s2 in range(S):\n",
    "                beta[i, s1] += beta[i+1, s2] * A[s1, s2] * O[s2, observations[i+1]]\n",
    "    \n",
    "    return (beta, np.sum(pi * O[:, observations[0]] * beta[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm the it gets the same marginal probability as the forward algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "backward((pi, A, O), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised training of a HMM involves running forward and backward to estimate the *expected* probability of taking various state sequences, then updates the model to match these *expectations*. This repeats many times until things stabilise (covergence). Note that it's non-convex, so the starting point often affects the converged solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def baum_welch(training, pi, A, O, iterations):\n",
    "    pi, A, O = np.copy(pi), np.copy(A), np.copy(O) # take copies, as we modify them\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    # do several steps of EM hill climbing\n",
    "    for it in range(iterations):\n",
    "        pi1 = np.zeros_like(pi)\n",
    "        A1 = np.zeros_like(A)\n",
    "        O1 = np.zeros_like(O)\n",
    "        \n",
    "        for observations in training:\n",
    "            # compute forward-backward matrices\n",
    "            alpha, za = forward((pi, A, O), observations)\n",
    "            beta, zb = backward((pi, A, O), observations)\n",
    "            assert abs(za - zb) < 1e-6, \"it's badness 10000 if the marginals don't agree\"\n",
    "            \n",
    "            # M-step here, calculating the frequency of starting state, transitions and (state, obs) pairs\n",
    "            pi1 += alpha[0,:] * beta[0,:] / za\n",
    "            for i in range(0, len(observations)):\n",
    "                O1[:, observations[i]] += alpha[i,:] * beta[i,:] / za\n",
    "            for i in range(1, len(observations)):\n",
    "                for s1 in range(S):\n",
    "                    for s2 in range(S):\n",
    "                        A1[s1, s2] += alpha[i-1,s1] * A[s1, s2] * O[s2, observations[i]] * beta[i,s2] / za\n",
    "                                                                    \n",
    "        # normalise pi1, A1, O1\n",
    "        pi = pi1 / np.sum(pi1)\n",
    "        for s in range(S):\n",
    "            A[s, :] = A1[s, :] / np.sum(A1[s, :])\n",
    "            O[s, :] = O1[s, :] / np.sum(O1[s, :])\n",
    "    \n",
    "    return pi, A, O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out by training on our example from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi2, A2, O2 = baum_welch([[UP, UP, DOWN]], pi, A, O, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi2, A2, O2), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it memorised the sequence, and has assigned it a very high probability. The downside is that it won't be very accepting of other sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi2, A2, O2), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks strangely reminiscent of many other learning problems.... Can you think how we might deal with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally training on both sequences leads to perhaps a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pi3, A3, O3 = baum_welch([[UP, UP, DOWN], [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP]], pi, A, O, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi3, A3, O3), [UP, UP, DOWN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward((pi3, A3, O3), [UP, UP, DOWN, UNCHANGED, UNCHANGED, DOWN, UP, UP])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping (making?) in real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a toy implementation of a HMM for pedagogical purposes. In reality we use several tricks to make things faster (e.g., matrix-vector operations) and to avoid floating point issues of underflow. These tricks complicate the code a fair bit. See the [Rabiner tutorial](http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf) for details, especifically the section about scaling factors. Another trick is to [work in log-space](http://machineintelligence.tumblr.com/post/4998477107/the-log-sum-exp-trick), which is easy for Viterbi but a bit more painful (and slower) for forward-backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises?\n",
    "\n",
    " - NLTK, currently has HMM implemeneted. Can you train the HMM with brown corpus with different training sizes:\n",
    "    - 5000 sentences.\n",
    "    - 10000 sentences\n",
    "    - 15000 sentences\n",
    "    - 20000 sentences\n",
    "    \n",
    " - Plot the difference in accuracy for a set of held out dataset, for training set of varying sizes.\n",
    " - Plot the difference in time taken for training with varying sizes of training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trevor Cohn course page-: http://people.eng.unimelb.edu.au/tcohn/comp90042.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developer - Pranav Shukla<br>\n",
    "Email id - pranavdynamic@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
