{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Details\n",
    "\n",
    "### AIM : \n",
    "<b> To implement a language model which will perform better in playing the Hangman game as compared to the traditional models which depend on unigram,bigram and n-gram models</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus Description\n",
    "##### Training Corpus\n",
    "<ul>\n",
    "<li>Name: <b> train.txt.</b></li>\n",
    "<li>Description: <b> Contains 753064 lowercase english words</b></li>\n",
    "</ul>\n",
    "\n",
    "##### Testing Corpus\n",
    "<ul>\n",
    "<li>Name: <b> test.txt.</b></li>\n",
    "<li>Description: <b> Contains 11118 lowercase english words</b></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file takes 0.845048904419 seconds\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "words = []\n",
    "start_time = time.time()\n",
    "with open(\"train.txt\", mode=\"r\") as myFile:\n",
    "    for line in myFile:\n",
    "        try:\n",
    "            words.append(line.strip().encode(\"utf-8\"))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print \"Reading the file takes %s seconds\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating alphabet probabilities in the list of words\n",
    "\n",
    "This function returns the probability s of a particular alphabet, calculated on the basis of \n",
    "its occurence in the word list passed it to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "def count_alpha_occurence(list_of_words):\n",
    "    start_time = time.time()\n",
    "    freq_dict = [0]*26\n",
    "    total = 0\n",
    "    for word in list_of_words:\n",
    "        for char in word:\n",
    "            total += word.count(char)\n",
    "            freq_dict[ord(char)-ord('a')] = freq_dict[ord(char)-ord('a')] + word.count(char)\n",
    "\n",
    "    for i in xrange(len(freq_dict)):\n",
    "        freq_dict[i] = float(freq_dict[i])/total\n",
    "    print \"Calculating alphabet freuency %s seconds\" % (time.time() - start_time)\n",
    "    return freq_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the conditional probabilties of an alphabet\n",
    " For a given a alphabet 'a' it returns a dictionary where key is 'b' and value is the probability of b \n",
    " occuring in a word given that 'a' is present in the word. This is to exploit the fact that occurence \n",
    " of an alphabet depends on the company it keeps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def conditional_prob(words):\n",
    "    start_time = time.time()\n",
    "    letter_freq = {}\n",
    "    prob = {}\n",
    "    for i in string.lowercase:\n",
    "        prob[i] = {}\n",
    "        for word in words:\n",
    "            if str(i) in word:\n",
    "                if i in letter_freq:\n",
    "                    letter_freq[i] +=1\n",
    "                else:\n",
    "                    letter_freq[i] = 1\n",
    "                for char in word:\n",
    "                    if char == i:\n",
    "                        continue\n",
    "                    if i in prob:\n",
    "                        if char in prob[i]:\n",
    "                            prob[i][char] += 1\n",
    "                        else:\n",
    "                            prob[i][char] = 1\n",
    "                    else:\n",
    "                        prob[i][char] = 1\n",
    "    for key in prob.keys():\n",
    "         for inner_key in prob[key].keys():\n",
    "            prob[key][inner_key] = float(prob[key][inner_key])/letter_freq[key]\n",
    "    print \"Conditional Probability Calculation %s seconds\" % (time.time() - start_time)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function\n",
    "This function checks whether the guessed letter exists in the word and returns the number of\n",
    "wrong guesses done to predict the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "guesses_for_this_word = []\n",
    "def hangman(secret_word, guesser, max_mistakes=8, verbose=True):\n",
    "    global guesses_for_this_word\n",
    "    guesses_for_this_word = []\n",
    "    secret_word = secret_word.lower()\n",
    "    mask = ['_'] * len(secret_word)\n",
    "    guessed = set()\n",
    "    if verbose:\n",
    "        print(\"Starting hangman game. Target is\", ' '.join(mask), 'length', len(secret_word))\n",
    "    \n",
    "    mistakes = 0\n",
    "    while mistakes < max_mistakes:\n",
    "        if verbose:\n",
    "            print(\"You have\", (max_mistakes-mistakes), \"attempts remaining.\")\n",
    "        guess = guesser(mask, guessed)\n",
    "\n",
    "        if verbose:\n",
    "            print('Guess is', guess)\n",
    "        if guess in guessed:\n",
    "            if verbose:\n",
    "                print('Already guessed this before.')\n",
    "            mistakes += 1\n",
    "        else:\n",
    "            try:\n",
    "                guessed.add(guess)\n",
    "            except:\n",
    "                print(guessed,guess)\n",
    "            if guess in secret_word:\n",
    "                for i, c in enumerate(secret_word):\n",
    "                    if c == guess:\n",
    "                        mask[i] = c\n",
    "                if verbose:\n",
    "                    print('Good guess:', ' '.join(mask))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Sorry, try again.')\n",
    "                mistakes += 1\n",
    "                \n",
    "        if '_' not in mask:\n",
    "            if verbose:\n",
    "                print('Congratulations, you won.')\n",
    "            return mistakes\n",
    "        \n",
    "    if verbose:\n",
    "        print('Out of guesses. The word was', secret_word)\n",
    "    guesses_for_this_word = list(guessed)\n",
    "    return mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Choice\n",
    "<p> One of the ways to prove the fact that a problem is tough is see how it performs on random inputs. Here in this baseline model, we predict alphabets randomly and see how many can we get correct. I have ensured that the same letter is not repeated again</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randomly(mask,guesses):\n",
    "    char = numpy.random.choice(list(string.lowercase))\n",
    "    while char in guesses:\n",
    "        char = numpy.random.choice(list(string.lowercase))\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a dictionary with alphabets as keys and their co-occurence as value, this function will normalize values such\n",
    "that they sum upto one\n",
    "\"\"\"\n",
    "def normalize(co_occ_dict):\n",
    "    total = 0\n",
    "    for key in co_occ_dict:\n",
    "        total += co_occ_dict[key]\n",
    "    for key in co_occ_dict:\n",
    "        co_occ_dict[key] = co_occ_dict[key]/float(total)\n",
    "    return co_occ_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating alphabet freuency 4.28961992264 seconds\n",
      "Conditional Probability Calculation 12.2717518806 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The number of times an alphabet occurs is calculated and also for each alphabet its co-occurence is calculated\n",
    "\"\"\"\n",
    "import numpy\n",
    "occurences = count_alpha_occurence(words)\n",
    "co_occurences = conditional_prob(words)\n",
    "prev_guess = 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model - 1\n",
    "<p> In this model I select an alphabet for the first spot with a distrubution given by its occurence. For eg if I have to choose from the alphabets <code>['a','b','c']</code> with distribution <code>[0.6,0.3,0.1]</code> then I will choose 'a' with 60% chance, 'b' with 30% chance and 'c' with 10% chance</p>\n",
    "<p> For all other alphabets I do the same wrt to previous guess's distribution</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def model_1(mask,guesses):\n",
    "    global occurences,prev_guess\n",
    "    if len(mask) == mask.count('_'):\n",
    "        char = numpy.random.choice(list(string.lowercase),1,occurences)[0]\n",
    "        while char in guesses:\n",
    "            char = numpy.random.choice(list(string.lowercase),1,occurences)[0]\n",
    "        prev_guess = char\n",
    "        return char\n",
    "    else:\n",
    "        temp = normalize(co_occurences[prev_guess])\n",
    "        temp = sorted(temp.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        cdf  = [(i, sum(p for j,p in temp if j < i)) for i,_ in temp]\n",
    "        char = max(i for r in [random.random()] for i,c in cdf if c <= r)\n",
    "        while char in guesses:\n",
    "            char = max(i for r in [random.random()] for i,c in cdf if c <= r)\n",
    "        prev_guess = char\n",
    "        return char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model - 2\n",
    "<p> In this model, for the first spot I choose an alphabet in the decreasing order of their occurence until I guess one correctly</p> \n",
    "<p> For all other alphabets,same as model - 1 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def model_2(mask,guesses):\n",
    "    global occurences,prev_guess\n",
    "    copy = occurences[:]\n",
    "    if len(mask) == mask.count('_'):\n",
    "        temp = copy.index(max(copy))\n",
    "        copy[temp] = -1\n",
    "        char = chr(temp+97)\n",
    "        while char in guesses:\n",
    "            temp = copy.index(max(copy))\n",
    "            copy[temp] = -1\n",
    "            char = chr(temp+97)\n",
    "        prev_guess = char\n",
    "        return char\n",
    "    else:\n",
    "        temp = normalize(co_occurences[prev_guess])\n",
    "        temp = sorted(temp.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        cdf  = [(i, sum(p for j,p in temp if j < i)) for i,_ in temp]\n",
    "        char = max(i for r in [random.random()] for i,c in cdf if c <= r)\n",
    "        while char in guesses:\n",
    "            char = max(i for r in [random.random()] for i,c in cdf if c <= r)\n",
    "        prev_guess = char\n",
    "        return char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model - 3\n",
    "<p> For first alphabet - same as model - 2</p>\n",
    "<p> For all other alphabets I do the same as model - 2 with difference that, I consider only top 10 for probabilistic transition.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_list(temp):\n",
    "    total = 0\n",
    "    for i in xrange(len(temp)):\n",
    "        total += temp[i]\n",
    "    for i in xrange(len(temp)):\n",
    "        temp[i] = float(temp[i])/total\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getMax5(temp):\n",
    "    alphas = []\n",
    "    values = []\n",
    "    for k,j in temp:\n",
    "        alphas.append(k)\n",
    "        values.append(j)\n",
    "    total = 0\n",
    "    value = normalize_list(values)\n",
    "    return values,alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_3(mask,guesses):\n",
    "    global occurences,prev_guess\n",
    "    copy = occurences[:]\n",
    "    if len(mask) == mask.count('_'):\n",
    "        temp = copy.index(max(copy))\n",
    "        copy[temp] = -1\n",
    "        char = chr(temp+97)\n",
    "        while char in guesses:\n",
    "            temp = copy.index(max(copy))\n",
    "            copy[temp] = -1\n",
    "            char = chr(temp+97)\n",
    "        prev_guess = char\n",
    "        return char\n",
    "    else:\n",
    "        temp = normalize(co_occurences[prev_guess])\n",
    "        temp = sorted(temp.items(), key=operator.itemgetter(1),reverse=True)[:10]\n",
    "        values,alphas = getMax5(temp)\n",
    "        char = numpy.random.choice(alphas,1,values)[0]\n",
    "        while char in guesses:\n",
    "            char = numpy.random.choice(alphas,1,values)[0]\n",
    "            ind = alphas.index(char)\n",
    "            values.remove(values[ind])\n",
    "            alphas.remove(char)\n",
    "            normalize_list(values)\n",
    "            if not alphas:\n",
    "                break\n",
    "        while char in guesses:\n",
    "            char = numpy.random.choice(list(string.lowercase))[0]\n",
    "        prev_guess = char\n",
    "        return char  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the file takes 0.0214998722076 seconds\n"
     ]
    }
   ],
   "source": [
    "test_words = []\n",
    "start_time = time.time()\n",
    "with open(\"test.txt\", mode=\"r\") as myFile:\n",
    "    for line in myFile:\n",
    "        try:\n",
    "            test_words.append(line.strip().encode(\"utf-8\"))\n",
    "        except:\n",
    "            pass\n",
    "print \"Reading the file takes %s seconds\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def test_model(model,filename):\n",
    "    mistakes = 0\n",
    "    correct = 0\n",
    "    performance = dict()\n",
    "    start_time = time.time()\n",
    "    for word in test_words:\n",
    "        temp= hangman(word,model,verbose=False)\n",
    "        if temp < 8:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes += temp\n",
    "        performance[word] = guesses_for_this_word\n",
    "\n",
    "    print \"Testing takes %s seconds\" % (time.time() - start_time)\n",
    "    print \"Correctly guessed words \" , correct\n",
    "\n",
    "    f = open(str(filename)+'.json','w')\n",
    "    json.dump(performance,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing takes 1.24915194511 seconds\n",
      "Correctly guessed words  73\n"
     ]
    }
   ],
   "source": [
    "test_model(randomly,\"Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing takes 10.3432209492 seconds\n",
      "Correctly guessed words  800\n"
     ]
    }
   ],
   "source": [
    "test_model(model_1,\"Model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing takes 10.8504860401 seconds\n",
      "Correctly guessed words  1019\n"
     ]
    }
   ],
   "source": [
    "test_model(model_2,\"Model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing takes 12.2866559029 seconds\n",
      "Correctly guessed words  2070\n"
     ]
    }
   ],
   "source": [
    "test_model(model_3,\"Model_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Details\n",
    "\n",
    "<ol>\n",
    "    <li>Name: <b>G V Sandeep</b></li>\n",
    "    <li>College: <b>BITS - Pilani, Hyderabad Campus</b></li>\n",
    "    <li>Github: <a href=\"https://github.com/greetsandeep/\">greetsandeep</a></li>\n",
    "</ol>\n",
    "\n",
    "This code is open sourced and can be found at : <a href\"https://github.com/greetsandeep/ACM_SummerSchool/tree/master/Improvising%20Hangman\">Improvising Hangman</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
